{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHBpVYpWUYz"
      },
      "source": [
        "## Gerekliliklerin Projeye Dahil Edilmesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPKEXzrrYQG",
        "outputId": "b3267226-33da-4350-e5ce-6e8af8dbf040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK]\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers &> /dev/null\n",
        "!pip install sentencepiece &> /dev/null\n",
        "!pip install sentence_splitter &> /dev/null\n",
        "print(\"[OK]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KigSqcDLHXxL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, LabelField \n",
        "from torchtext.legacy.data import TabularDataset, BucketIterator"
      ],
      "metadata": {
        "id": "7VyfOgIs_ZLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB8ssN-8HgmN"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDONXap9Z0LI"
      },
      "source": [
        "## Veri Kümesinden Örnek Seçimi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mifqobswaGT5"
      },
      "outputs": [],
      "source": [
        "class SampleSelection(object):\n",
        "  def __init__(self, min_sample_count=1000):\n",
        "    self.min_sample_count = min_sample_count\n",
        "\n",
        "  def generate_sub_set(self, ds):\n",
        "    eval_str = f\"ds.dataframe.{ds.label}.value_counts()\"\n",
        "    label_count = eval(eval_str)\n",
        "    keys = label_count.keys()\n",
        "    counts = np.asarray(label_count) / np.sum(label_count) * self.min_sample_count + 1\n",
        "    counts = np.asarray(counts, dtype=int)\n",
        "    print(f\"Dataset Name     : {ds.name}\\n\"+\n",
        "          f\"Total count      : {np.sum(label_count)}\\n\"+\n",
        "          f\"Distribution     : {np.asarray(label_count)}\\n\"+\n",
        "          f\"Current count    : {np.sum(counts)}\\n\"+\n",
        "          f\"New Distribution : {counts}\\n\"+\n",
        "          f\"Label Names      : {list(keys)}\\n\"+\n",
        "          f\"Num of Label     : {len(keys)}\\n\\n\")\n",
        "    return self._generate(ds, keys, counts)\n",
        "\n",
        "  def _generate(self, ds, keys, counts):\n",
        "    new_df = pd.DataFrame(columns=[ds.text, ds.label])\n",
        "    for i in range(len(keys)):\n",
        "      temp = ds.dataframe[ds.dataframe[ds.label]==keys[i]].sample(counts[i], replace=False)\n",
        "      new_df = new_df.append(temp, ignore_index=True)\n",
        "    return new_df\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_WQcj78IaYt"
      },
      "source": [
        "## Veri Kümelerinin Normalizasyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGb7mgABLnfM"
      },
      "outputs": [],
      "source": [
        "class DatasetBase(object):\n",
        "  def __init__(self, name, text, label, file_id, dataframe=None, coding=\"utf-8\"):\n",
        "    self.name = name\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.file_id = file_id\n",
        "    self.dataframe = dataframe\n",
        "    self.coding = coding\n",
        "\n",
        "    self.train_set = None\n",
        "    self.test_set = None\n",
        "    self.paraphrased_t5_set = None\n",
        "    self.paraphrased_pg_set = None\n",
        "    self.norm_set = None\n",
        "\n",
        "  def set_dataframe(self, dataframe): \n",
        "    self.dataframe = dataframe\n",
        "\n",
        "  def read_dataframe(self):\n",
        "    self.dataframe = pd.read_csv(self.name, encoding=self.coding)\n",
        "\n",
        "  def save_dataframe(self, name=None, prefix=None):\n",
        "    if name==None: name = self.name\n",
        "    if prefix==None: prefix = \"Melora_Hardin\"\n",
        "    self.dataframe.to_csv(f\"{prefix}_{name}\")\n",
        "\n",
        "  def download_dataframe(self, drive):\n",
        "    downloaded = drive.CreateFile({'id': self.file_id})\n",
        "    downloaded.GetContentFile(self.name)\n",
        "    self.read_dataframe()\n",
        "\n",
        "class DatasetNormalization(object):\n",
        "  def __init__(self, text=\"text\", label=\"label\"):\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "\n",
        "  def get_features(self): return self.text, self.label\n",
        "\n",
        "  def get_normalized_ds(self, dataset):\n",
        "    dataframe =  dataset.dataframe.filter(items=[dataset.text, dataset.label])\n",
        "    dataframe.rename({dataset.text: self.text, dataset.label: self.label}, axis=1, inplace=True)\n",
        "    dataframe = dataframe[[self.text, self.label]]\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eğitim - Test Veri Kümelerinin Tanımlanması  "
      ],
      "metadata": {
        "id": "Uf25dWgjJBiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateTrainTestSet(object):\n",
        "  def __init__(self,ds, norm_text=\"text\", norm_label=\"label\"):\n",
        "    self.ds = ds\n",
        "    self.ds_df = pd.read_csv(f\"ds_{ds.name}\", encoding=ds.coding)\n",
        "    #self.ds_df.columns = [norm_label, norm_text]\n",
        "    self.ds.dataframe = self.ds_df\n",
        "    self.norm_df = pd.read_csv(f\"norm_{ds.name}\")\n",
        "    self.t5_df = pd.read_csv(f\"t5_{ds.name}\")\n",
        "    self.pg_df = pd.read_csv(f\"pg_{ds.name}\")\n",
        "    self.norm_text = norm_text\n",
        "    self.norm_label = norm_label\n",
        "    # outputs\n",
        "    # normal train and test set : ds - norm % per -> train + norm, test\n",
        "    # pharaphrasing set         : ph % max k -> select k row by group max k row \n",
        "\n",
        "\n",
        "  def get_normal_set(self, per=0.4):\n",
        "    normalization = DatasetNormalization()\n",
        "    self.ds_df = normalization.get_normalized_ds(self.ds)\n",
        "    self.ds.dataframe = self.ds_df\n",
        "    self.ds.text = self.norm_text\n",
        "    self.ds.label = self.norm_label\n",
        "\n",
        "    eval_str = f\"self.ds_df.{self.norm_text}\"\n",
        "    for index, row in self.norm_df.iterrows():  # sub set in train setten çıkarılması\n",
        "      self.ds_df = self.ds_df.drop(self.ds_df[eval(eval_str)==row[self.norm_text]].index)\n",
        "    print(\"Training Set\")\n",
        "\n",
        "    test_set = SampleSelection(min_sample_count=self.ds_df.shape[0] * per).generate_sub_set(self.ds)\n",
        "    for index, row in test_set.iterrows():      # test set in train setten çıkarılması\n",
        "      self.ds_df = self.ds_df.drop(self.ds_df[eval(eval_str)==row[self.norm_text]].index)\n",
        "    for index, row in self.norm_df.iterrows():  # sub setin train sete eklenmesi\n",
        "      self.ds_df = self.ds_df.append({self.ds.text:row[self.norm_text], \n",
        "                                      self.ds.label:row[self.norm_label]}, ignore_index=True)\n",
        "    print(f\"Train Set : {self.ds_df.shape[0]}\\n\"+\n",
        "          f\"Test Set  : {test_set.shape[0]}\\n\"+\n",
        "          f\"Norm Set  : {self.norm_df.shape[0]}\\n\")\n",
        "    return self.ds_df, test_set, self.norm_df\n",
        "\n",
        "\n",
        "\n",
        "  def get_paraphrased_set(self, per=[0.2, 0.5, 0.7, 1, 2, 5], k=5, prefix=\"t5\"):\n",
        "    less_one = list()\n",
        "    less_per = list()\n",
        "    one = pd.DataFrame(columns=[self.norm_text, self.norm_label])\n",
        "    greater_one = list()\n",
        "    greater_per = list()\n",
        "\n",
        "    paraphrased_dataset = pd.read_csv(f\"{prefix}_{self.ds.name}\")\n",
        "\n",
        "    for p in per:\n",
        "      if p>=1: \n",
        "        greater_one.append(pd.DataFrame(columns=[self.norm_text, self.norm_label]))\n",
        "        greater_per.append(p)\n",
        "    \n",
        "    # 1 ve 1 den büyük olanların seçilmesi\n",
        "    temp_df = pd.DataFrame(columns=[self.norm_text, self.norm_label])\n",
        "    for index, row in paraphrased_dataset.iterrows():\n",
        "      temp_df = temp_df.append({self.norm_text:row[self.norm_text],\n",
        "                                  self.norm_label:row[self.norm_label]}, ignore_index=True)\n",
        "      if (index+1) % k == 0: # 5 lik row alındı\n",
        "        \n",
        "        for temp_index, temp_row in temp_df.iterrows():\n",
        "          greater_index = 0\n",
        "          for p in per: \n",
        "            if p>=1: \n",
        "              if temp_index < p:\n",
        "                greater_one[greater_index] = greater_one[greater_index].append({self.norm_text:temp_row[self.norm_text],\n",
        "                                      self.norm_label:temp_row[self.norm_label]}, ignore_index=True)\n",
        "              greater_index += 1\n",
        "                \n",
        "          if temp_index == 0: \n",
        "            one = one.append({self.norm_text:temp_row[self.norm_text],\n",
        "                                  self.norm_label:temp_row[self.norm_label]}, ignore_index=True)\n",
        "          \n",
        "        temp_df = pd.DataFrame(columns=[self.norm_text, self.norm_label])\n",
        "        \n",
        "        \n",
        "    \n",
        "    # 1 den düşük olan oranların seçilmesi\n",
        "    temp_df = copy.deepcopy(self.ds)\n",
        "    temp_df.dataframe = one\n",
        "    temp_df.text = self.norm_text\n",
        "    temp_df.label = self.norm_label\n",
        "    \n",
        "    for p in per: \n",
        "      if p<1: \n",
        "        print(p, temp_df.dataframe.shape[0])\n",
        "        less_one.append(\n",
        "            SampleSelection(min_sample_count=self.norm_df.shape[0]*p).\n",
        "            generate_sub_set(temp_df)\n",
        "        )\n",
        "\n",
        "    output = list()\n",
        "    i = 0\n",
        "    print(\"Percentages and Numbers of Paraphrased Rows\")\n",
        "    for less in less_one:\n",
        "      print(f\"\\t{per[i]} -> {less.shape[0]}\")\n",
        "      i += 1\n",
        "      output.append(less)\n",
        "    for greater in greater_one:\n",
        "      print(f\"\\t{per[i]} -> {greater.shape[0]}\")\n",
        "      i += 1\n",
        "      output.append(greater)\n",
        "    return output\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "3rCxHpB4JUD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE995QzZqhMX"
      },
      "source": [
        "## T5 ve Pegasus Mimarilerinin Tanımlanması"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gK8xr9gqk5e"
      },
      "outputs": [],
      "source": [
        "class Paraphrase_T5(object):\n",
        "  '''\n",
        "    source: https://huggingface.co/hetpandya/t5-small-tapaco\n",
        "  '''\n",
        "  def __init__(self, device=\"cpu\"):\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained(\"hetpandya/t5-small-tapaco\")\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(\"hetpandya/t5-small-tapaco\")\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def get(self, sentence, prefix=\"paraphrase: \", n_predictions=5, \n",
        "                      top_k=120, max_length=256, ):\n",
        "    text = prefix + sentence + \" </s>\"\n",
        "    encoding = self.tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"].to(self.device), encoding[\"attention_mask\"].to(self.device)\n",
        "\n",
        "    model_output = self.model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_masks,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        top_k=top_k,\n",
        "        top_p=0.98,\n",
        "        early_stopping=True,\n",
        "        num_return_sequences=n_predictions,\n",
        "    )\n",
        "    return self._get(sentence, model_output, n_predictions)\n",
        "    outputs = []\n",
        "    for output in model_output:\n",
        "      generated_sent = self.tokenizer.decode(\n",
        "        output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "      )\n",
        "      if (\n",
        "        generated_sent.lower() != sentence.lower()\n",
        "        and generated_sent not in outputs\n",
        "        ):\n",
        "        outputs.append(generated_sent)\n",
        "    return outputs\n",
        "\n",
        "  def _get(self, sentence, model_output, n):\n",
        "    outputs = []\n",
        "    for i in range(3):\n",
        "      for output in model_output:\n",
        "          generated_sent = self.tokenizer.decode(\n",
        "              output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "          )\n",
        "          if (\n",
        "              generated_sent.lower() != sentence.lower()\n",
        "              and generated_sent not in outputs\n",
        "          ):\n",
        "              outputs.append(generated_sent)\n",
        "          if len(outputs) == n: return outputs\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRwdzoDJuqmW"
      },
      "outputs": [],
      "source": [
        "class Paraphrase_Pegasus(object):\n",
        "  '''\n",
        "    source: https://analyticsindiamag.com/how-to-paraphrase-text-using-pegasus-transformer/\n",
        "  '''\n",
        "  def __init__(self, device=\"cpu\"):\n",
        "    self.tokenizer = PegasusTokenizer.from_pretrained('tuner007/pegasus_paraphrase')\n",
        "    self.model = PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase').to(device)\n",
        "    self.splitter = SentenceSplitter(language='en')\n",
        "    self.device = device\n",
        "\n",
        "  def get(self, sentence, n_predictions=5, num_beams=10, max_length=256, ):\n",
        "    sentence_list = self.splitter.split(sentence)\n",
        "    output = []\n",
        "    for i in range(n_predictions): output.append(\"\")\n",
        "    for sentence in sentence_list:\n",
        "      batch = self.tokenizer([sentence],truncation=True,padding='longest',max_length=max_length, return_tensors=\"pt\").to(self.device)\n",
        "      translated = self.model.generate(**batch,max_length=max_length,num_beams=num_beams, num_return_sequences=n_predictions, temperature=1.5)\n",
        "      tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "      for i in range(len(tgt_text)): output[i] += tgt_text[i] + \" \" \n",
        "    return output\n",
        "\n",
        "  def _get(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkG4ilk8BEnq"
      },
      "source": [
        "## Yapay Örnek Üretimi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLf--oKOBJJn"
      },
      "outputs": [],
      "source": [
        "class ParaphraseBase(object):\n",
        "  def __init__(self, architectures, k=5):\n",
        "    self.k = k\n",
        "    self.architectures = architectures\n",
        "\n",
        "  def generate(self, ds):\n",
        "    new_df = list()\n",
        "    for arc in self.architectures: new_df.append(pd.DataFrame(columns=[ds.text, ds.label]))\n",
        "    total_row = ds.dataframe[ds.label].count() + 1\n",
        "    for curr_row, row in ds.dataframe.iterrows():\n",
        "      print(f\"\\t[{curr_row}/{total_row}]: %{curr_row/total_row*100}\")\n",
        "      sentence_error = False\n",
        "      for i_arch in range(len(self.architectures)):\n",
        "        try:\n",
        "          phrases = self.architectures[i_arch].get(row[ds.text], n_predictions=self.k)\n",
        "        except: \n",
        "          sentence_error = True\n",
        "          print(f\"[X] Paraphrase Error in {curr_row}\")\n",
        "          phrases = self._pseudo_phrase()\n",
        "        for phr in phrases:\n",
        "          new_df[i_arch] = new_df[i_arch].append({ds.text:phr, ds.label:row[ds.label]}, ignore_index=True)\n",
        "      if sentence_error: new_df = self._delete_last_sentences(new_df)\n",
        "    return new_df\n",
        "\n",
        "  def _pseudo_phrase(self):\n",
        "    phrases = list()\n",
        "    for i in range(self.k): phrases.append(\"Pseudo\")\n",
        "    return phrases\n",
        "\n",
        "  def _delete_last_sentences(self, df_list):\n",
        "    for df in df_list:\n",
        "      df = df.drop(df.tail(self.k).index,inplace=True)\n",
        "    return df_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eğitim ve Test Süreci"
      ],
      "metadata": {
        "id": "8aAVu_kP68DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "X7qW_PEJ8g9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "#stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def text_tokenization(text): \n",
        "    text = text.replace(\"n't\", ' not')\n",
        "    text = text.replace(\"'ve\", ' have')\n",
        "    text = text.replace(\"'m\", ' am')\n",
        "    text = text.replace(\"’s\", ' ')\n",
        "    text = text.replace(\"'s\", ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = re.sub(\"[..,,ㆍ\\'\\\"’‘”“!?\\\\‘|\\(\\)\\[\\]\\<\\>`\\'◇…]\", \"\", text)\n",
        "    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n",
        "    text = re.sub(\" +\", \" \", text) #remove multi-space\n",
        "    text = text.lower()\n",
        "    return \" \".join([stemmer.stem(word) for word in str(text).split() \n",
        "                     if word not in stop_words])\n",
        "\n",
        "def cnn_tokenization(text):\n",
        "    text = text.replace(\"n't\", ' not')\n",
        "    text = text.replace(\"'ve\", ' have')\n",
        "    text = text.replace(\"'m\", ' am')\n",
        "    text = text.replace(\"’s\", ' ')\n",
        "    text = text.replace(\"'s\", ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = re.sub(\"[..,,ㆍ\\'\\\"’‘”“!?\\\\‘|\\(\\)\\[\\]\\<\\>`\\'◇…]\", \"\", text)\n",
        "    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n",
        "    text = re.sub(\" +\", \" \", text) #remove multi-space\n",
        "    text = text.lower()\n",
        "    return [stemmer.stem(word) for word in str(text).split() \n",
        "                     if word not in stop_words]\n",
        "\n",
        "def only_lemmatization(text):\n",
        "    text = text.replace(\"n't\", ' not')\n",
        "    text = text.replace(\"'ve\", ' have')\n",
        "    text = text.replace(\"'m\", ' am')\n",
        "    text = text.replace(\"’s\", ' ')\n",
        "    text = text.replace(\"'s\", ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = re.sub(\"[..,,ㆍ\\'\\\"’‘”“!?\\\\‘|\\(\\)\\[\\]\\<\\>`\\'◇…]\", \"\", text)\n",
        "    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n",
        "    text = re.sub(\" +\", \" \", text) #remove multi-space\n",
        "    text = text.lower()\n",
        "    text = \" \".join([lemma.lemmatize(word, pos='v') for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdLHubHJ753t",
        "outputId": "cbc523eb-e6ba-421d-f305-fd07a69858ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "VHZgT9vl8l_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()           \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)       \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])       \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)    \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, text):     \n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]     \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1)) \n",
        "        return self.fc(cat)\n",
        "\n",
        "# Genel Metotlar\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, _ = batch.text\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def train_accuracy(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    #model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, _ = batch.text\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "          \n",
        "            text, _ = batch.text\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "          \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "x328NW6LKeqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train ve Test"
      ],
      "metadata": {
        "id": "VACtAX83M8Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_train(N_EPOCHS, model, train_iterator, optimizer, criterion, test_iterator):\n",
        "  train_acc_list, test_acc_list = [], []\n",
        "  best_valid_loss = float('inf')\n",
        "  epoch_weight = dict()\n",
        "\n",
        "  score_train = list()\n",
        "  score_test = list()\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "\n",
        "      start_time = time.time()\n",
        "      \n",
        "      train_loss, train_acc = train(model, \n",
        "                                    train_iterator, \n",
        "                                    optimizer, \n",
        "                                    criterion)\n",
        "      \n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      \n",
        "      test_loss, test_accuracy = evaluate(model, test_iterator, criterion)\n",
        "      \n",
        "      train_acc_list.append(train_acc*100)\n",
        "      test_acc_list.append(test_accuracy*100)\n",
        "\n",
        "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_acc*100:.2f}%')\n",
        "      print(f'\\tTest Loss :{test_loss} | Test Accuracy : {test_accuracy}')\n",
        "  return train_acc_list, test_acc_list\n",
        "  #torch.save(model.state_dict(), 'model_end.pt')\n"
      ],
      "metadata": {
        "id": "zbbuBm3RM-0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eğitim ve Test Süreci : Business Base"
      ],
      "metadata": {
        "id": "nCj0_JiO7A4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ModelTraining(object):\n",
        "  def __init__(self, train, test, norm):\n",
        "    self.epoch_3 = 4\n",
        "    self.epoch_2 = 6\n",
        "    self.epoch_1 = 12\n",
        "\n",
        "    self.test = self._get_dataset_to_be_used(test)\n",
        "\n",
        "    self._print(train)\n",
        "    #self.train = self._get_dataset_to_be_used(train)\n",
        "    #self._train_test(self.train, self.test, self.epoch_1, train)\n",
        "\n",
        "    self._print(norm)\n",
        "    self.norm = self._get_dataset_to_be_used(norm)\n",
        "    self._train_test(self.norm, self.test, self.epoch_1, norm)\n",
        "\n",
        "    self.norm_name = norm\n",
        "\n",
        "    \n",
        "  \n",
        "  def start_analysis_for_a_percentage(self, name, p):\n",
        "    # Önce -> 2\n",
        "    self._print(\"Önce Norm : \"+name)\n",
        "    if p>=0:\n",
        "      ds_norm = self._get_dataset_to_be_used(self.norm_name)\n",
        "      m = self._train_test(ds_norm, self.test, self.epoch_2, name + \" ÖNCE 1\")\n",
        "      ds_para = self._get_dataset_to_be_used(name)\n",
        "      self._train_test(ds_para, self.test, self.epoch_2, name + \" ÖNCE 2\", pretrained_model=m)\n",
        "      \n",
        "      \n",
        "      #m = self._train_test(ds_para, self.test, 3, name)\n",
        "      #self._train_test(ds_norm, self.test, 3, name, model=m)\n",
        "    #except: print(\"\\n\\t[X] ERROR because of INPUT SIZE of EMBEDDING_DIM\")\n",
        "\n",
        "    # Sonra -> 2\n",
        "    self._print(\"Sonra Norm : \"+name)\n",
        "    if p>=0:\n",
        "      \n",
        "      ds_para = self._get_dataset_to_be_used(name)\n",
        "      m = self._train_test(ds_para, self.test, self.epoch_2, name + \" SONRA 1\")\n",
        "      \n",
        "      ds_norm = self._get_dataset_to_be_used(self.norm_name)\n",
        "      self._train_test(ds_norm, self.test, self.epoch_2, name+ \" SONRA 2\", pretrained_model=m)\n",
        "    #except: print(\"\\n\\t[X] ERROR because of INPUT SIZE of EMBEDDING_DIM\")\n",
        "\n",
        "    # Karışık -> 1\n",
        "    if True:\n",
        "      self._print(\"Karışık : \"+name)\n",
        "      #try:\n",
        "      norm = pd.read_csv(self.norm_name)[[\"text\",\"label\"]]\n",
        "      para = pd.read_csv(name)[[\"text\",\"label\"]]\n",
        "      norm = norm.append(para, ignore_index=True)\n",
        "      norm.to_csv(\"temp_karisik.csv\")\n",
        "      ds_norm = self._get_dataset_to_be_used(\"temp_karisik.csv\")\n",
        "      self._train_test(ds_norm, self.test, self.epoch_1, name  + \" KARIŞIK\")\n",
        "      #except: print(\"\\n\\t[X] ERROR because of INPUT SIZE of EMBEDDING_DIM\")\n",
        "\n",
        "\n",
        "    # Arada -> 3\n",
        "    self._print(\"Arada : \"+name)\n",
        "    if p>=0:\n",
        "      #try:\n",
        "        para = pd.read_csv(name)[[\"text\",\"label\"]]\n",
        "        para1, para2 = train_test_split(para, test_size=0.5)\n",
        "        para1.to_csv(\"arada_ilk.csv\")\n",
        "        para2.to_csv(\"arada_son.csv\")\n",
        "\n",
        "        ds_first = self._get_dataset_to_be_used(\"arada_ilk.csv\")\n",
        "        m = self._train_test(ds_first, self.test, self.epoch_3, name + \" ARADA 1\")\n",
        "        \n",
        "        ds_norm = self._get_dataset_to_be_used(self.norm_name)\n",
        "        m = self._train_test(ds_norm, self.test, self.epoch_3, name + \" ARADA 2\", pretrained_model=m)\n",
        "\n",
        "        ds_last = self._get_dataset_to_be_used(\"arada_son.csv\")\n",
        "        self._train_test(ds_last, self.test, self.epoch_3, name + \" ARADA 3\", pretrained_model=m)\n",
        "      #except: print(\"\\n\\t[X] ERROR because of INPUT SIZE of EMBEDDING_DIM\")\n",
        "\n",
        "\n",
        "\n",
        "  def _train_test(self, train, test, epoch, name, pretrained_model=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "      (train, test), sort = False, repeat = False,\n",
        "      sort_within_batch =False, batch_size = 32, device = device)\n",
        "    \n",
        "    INPUT_DIM = len(self.TEXT.vocab)\n",
        "    EMBEDDING_DIM = 100\n",
        "    N_FILTERS = 100\n",
        "    FILTER_SIZES = [3,4,5]\n",
        "    OUTPUT_DIM = len(self.LABEL.vocab)\n",
        "    DROPOUT = 0.5\n",
        "    PAD_IDX = self.TEXT.vocab.stoi[self.TEXT.pad_token]\n",
        "\n",
        "    \n",
        "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "    if pretrained_model != None:\n",
        "      model.convs[0] = pretrained_model.convs[0]\n",
        "      model.convs[1] = pretrained_model.convs[1]\n",
        "      model.convs[2] = pretrained_model.convs[2] \n",
        "      model.fc = pretrained_model.fc\n",
        "\n",
        "    import torch.optim as optim\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    train_acc, test_acc = perform_train(epoch, model, train_iterator, optimizer, criterion, test_iterator)\n",
        "    print(f\"\\n# Generated for {name}\")\n",
        "    print(\"train_acc = \",train_acc)\n",
        "    print(\"test_acc = \",test_acc)\n",
        "    print(\"\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  def _get_dataset_to_be_used(self, name):\n",
        "    self.TEXT = Field(sequential=True, tokenize=cnn_tokenization, lower = True, include_lengths=True)\n",
        "    self.LABEL = LabelField(dtype=torch.long)\n",
        "    Raw_datafields = [(\"Unnamed:0\", None), (\"text\", self.TEXT), (\"label\", self.LABEL)]\n",
        "    MAX_VOCAB_SIZE = 2000 #25000\n",
        "    dataset = data.TabularDataset(\n",
        "        path=f\"./{name}\",\n",
        "        format='csv',\n",
        "        skip_header=True, \n",
        "        fields=Raw_datafields) \n",
        "    self.TEXT.build_vocab(dataset, max_size = MAX_VOCAB_SIZE)\n",
        "    self.LABEL.build_vocab(dataset)\n",
        "    print(f\"DATASET: {name}\")\n",
        "    print(vars(dataset[0]))\n",
        "    print(self.TEXT.vocab.freqs.most_common(30))\n",
        "    return dataset\n",
        "\n",
        "  def _print(self, text):\n",
        "    print(\"\\n\\n\\n\\n################################################################################################\\n\")\n",
        "    print(f\"[\\_(O.O)_/] {text}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "LB5Z3LzW7SNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmU9HeMEWFex"
      },
      "source": [
        "## Proje Prosedürünün Yürütülmesi - Yapay Örnek Üretimi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aUJhCy2Hj2-"
      },
      "outputs": [],
      "source": [
        "# Datasetlerin tanımlaması - Eğer bir dataset ilk kez normalize edilecekse burası çalıştırılmalıdır\n",
        "datasets = [\n",
        "            #DatasetBase(\"news_category.csv\", \"short_description\", \"category\", \"1-y9Xf_q0QSmEGMYlI8wUhusMjyktzJxZ\")\n",
        "            # X DatasetBase(\"Corona_NLP_train.csv\", \"OriginalTweet\", \"Sentiment\", \"1usKN2yV6Wm7RftLJv-xsNTMysdEht9gO\", coding=\"latin-1\")\n",
        "            #DatasetBase(\"tweets_disaster.csv\", \"text\", \"target\", \"1iMQCrNtDOTlea6j3ZdYe-JGs6R3MsOj8\")\n",
        "            #DatasetBase(\"financial_news_sentiment.csv\", \"text\", \"label\", \"1ngYA4Z7JomUhUZ6iUgcn-isj_-44VNLa\", coding='cp437')\n",
        "            DatasetBase(\"tweet_emotions_category.csv\", \"content\", \"sentiment\", \"1FDgaHTeF3wu6-B6jCfI5lBZJ0bB14zKb\")\n",
        "]\n",
        "\n",
        "# Datasetlerin yüklenmesi ve normalizasyonu\n",
        "normalization = DatasetNormalization()\n",
        "for ds in datasets:\n",
        "  ds.download_dataframe(drive)\n",
        "  ds.set_dataframe(normalization.get_normalized_ds(ds))\n",
        "  ds.text, ds.label = normalization.get_features()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxKs_q-ckBg",
        "outputId": "b26038dd-7d20-4a0c-ca32-ded22be485ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Name     : tweet_emotions_category.csv\n",
            "Total count      : 40000\n",
            "Distribution     : [8638 8459 5209 5165 3842 2187 1776 1526 1323  827  759  179  110]\n",
            "Current count    : 1007\n",
            "New Distribution : [216 212 131 130  97  55  45  39  34  21  19   5   3]\n",
            "Label Names      : ['neutral', 'worry', 'happiness', 'sadness', 'love', 'surprise', 'fun', 'relief', 'hate', 'empty', 'enthusiasm', 'boredom', 'anger']\n",
            "Num of Label     : 13\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Datasetlerden alt kümelerin elde edilmesi\n",
        "sampling = SampleSelection()\n",
        "for ds in datasets:\n",
        "  ds.dataframe = sampling.generate_sub_set(ds)\n",
        "  ds.save_dataframe(prefix=\"norm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zdh4jqBXErb"
      },
      "outputs": [],
      "source": [
        "# T5 ve Pegasus mimarilerinin ilklendirilmesi. \n",
        "# Not: Sadece bir kez çalıştırın. Her yürütme farklı hafıza blokları kullanır\n",
        "# Aşağıdaki yapılar üzerinde düzenleme yaptıktan sonra bir sonraki garbage collector çalıştırılmalı\n",
        "t5 = Paraphrase_T5()\n",
        "pegasus = Paraphrase_Pegasus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of-p2vuwOmNO"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del t5\n",
        "del pegasus\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7-YqgzCA7c4"
      },
      "outputs": [],
      "source": [
        "# Yapay örnek üretimi ve türetilmiş datasetlerin kaydedilmesi\n",
        "architectures = [t5, pegasus]\n",
        "paraphrase = ParaphraseBase(architectures)\n",
        "for ds in datasets:\n",
        "  new_df = paraphrase.generate(ds)\n",
        "  new_df[0].to_csv(f\"t5_{ds.name}\")\n",
        "  new_df[1].to_csv(f\"pg_{ds.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "for ds in datasets:\n",
        "  new_df[0].to_csv(f\"/content/drive/My Drive/t5_{ds.name}\")\n",
        "  new_df[1].to_csv(f\"/content/drive/My Drive/pg_{ds.name}\")"
      ],
      "metadata": {
        "id": "jupUvg82yT_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proje Prosedürünün Yürütülmesi - Veri Setlerin Hazırlanması"
      ],
      "metadata": {
        "id": "vwhPlaU8BPNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 li parçalar halinde dataframe rowlarını ayır, her 5 lik parça bir sentencedir\n",
        "# 1, 2 ve 5 inin alındığı kısımları oluştur\n",
        "# 1 lik kısımların %20, %50 sini seçip yeni bir set oluştur \n",
        "# kullanılan datasetlerin test setlerini oluştur\n",
        "# teker teker eğitmek için gerekli yapıyı kur, her epochtaki train test al"
      ],
      "metadata": {
        "id": "6mU8LT2S8NlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bu bölümde datasetlerin train-test olarak ayrılması ve eğitim yer alır\n",
        "downloaded = drive.CreateFile({'id': '1teFlXpOe0DKJoZStw3ORXpVKSN0O-x3y'})\n",
        "downloaded.GetContentFile(\"paraphrasing_dataset.rar\")\n",
        "!unrar e paraphrasing_dataset.rar"
      ],
      "metadata": {
        "id": "HQHiOnVmAUcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "            #DatasetBase(\"news_category.csv\", \"short_description\", \"category\", \"1-y9Xf_q0QSmEGMYlI8wUhusMjyktzJxZ\"),\n",
        "            #DatasetBase(\"Corona_NLP_train.csv\", \"OriginalTweet\", \"Sentiment\", \"1usKN2yV6Wm7RftLJv-xsNTMysdEht9gO\", coding=\"latin-1\"),\n",
        "            DatasetBase(\"tweets_disaster.csv\", \"text\", \"target\", \"1iMQCrNtDOTlea6j3ZdYe-JGs6R3MsOj8\"),\n",
        "            #DatasetBase(\"financial_news_sentiment.csv\", \"text\", \"label\", \"1ngYA4Z7JomUhUZ6iUgcn-isj_-44VNLa\", coding='cp437'),\n",
        "            #DatasetBase(\"tweet_emotions_category.csv\", \"content\", \"sentiment\", \"1FDgaHTeF3wu6-B6jCfI5lBZJ0bB14zKb\"),\n",
        "            #DatasetBase(\"google_appstore_reviews.csv\", \"content\", \"score\", \"1FDgaHTeF3wu6-B6jCfI5lBZJ0bB14zKb\")\n",
        "]\n",
        "\n",
        "paraphrase_percent = [0.2, 0.5, 0.7, 1, 2, 5]\n",
        "k = 5\n",
        "per=0.3\n",
        "\n",
        "def save_paraphrased_set(name, phr_set, percent, k, arch_name):\n",
        "  for i in range(len(percent)):\n",
        "    phr_set[i].to_csv(f\"phr_{arch_name}_{name}_k{k}_p{percent[i]}.csv\")\n",
        "\n",
        "for ds in datasets:\n",
        "  set_generation = GenerateTrainTestSet(ds)\n",
        "\n",
        "  paraphrased_t5_set = set_generation.get_paraphrased_set(per=paraphrase_percent, k=k, prefix=\"t5\")\n",
        "  save_paraphrased_set(ds.name, paraphrased_t5_set, paraphrase_percent, k, \"t5\")\n",
        "\n",
        "  paraphrased_pg_set = set_generation.get_paraphrased_set(per=paraphrase_percent, k=k, prefix=\"pg\")\n",
        "  save_paraphrased_set(ds.name, paraphrased_pg_set, paraphrase_percent, k, \"pg\")\n",
        "\n",
        "  train_set, test_set, norm_set = set_generation.get_normal_set(per=per)\n",
        "  train_set.to_csv(f\"train_per{per}_{ds.name}\")\n",
        "  test_set.to_csv(f\"test_per{per}_{ds.name}\")\n",
        "\n",
        "  ds.train_set = train_set\n",
        "  ds.test_set = test_set\n",
        "  ds.paraphrased_t5_set = paraphrased_t5_set\n",
        "  ds.paraphrased_pg_set = paraphrased_pg_set\n",
        "  ds.norm_set = norm_set\n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xQ6BU7rBzkr",
        "outputId": "1967926a-bc85-4de0-ce81-9089f0923636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2 959\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 959\n",
            "Distribution     : [546 413]\n",
            "Current count    : 201\n",
            "New Distribution : [114  87]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "0.5 959\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 959\n",
            "Distribution     : [546 413]\n",
            "Current count    : 501\n",
            "New Distribution : [285 216]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "0.7 959\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 959\n",
            "Distribution     : [546 413]\n",
            "Current count    : 701\n",
            "New Distribution : [399 302]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "Percentages and Numbers of Paraphrased Rows\n",
            "\t0.2 -> 201\n",
            "\t0.5 -> 501\n",
            "\t0.7 -> 701\n",
            "\t1 -> 959\n",
            "\t2 -> 1918\n",
            "\t5 -> 4795\n",
            "0.2 969\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 969\n",
            "Distribution     : [555 414]\n",
            "Current count    : 201\n",
            "New Distribution : [115  86]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "0.5 969\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 969\n",
            "Distribution     : [555 414]\n",
            "Current count    : 501\n",
            "New Distribution : [287 214]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "0.7 969\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 969\n",
            "Distribution     : [555 414]\n",
            "Current count    : 702\n",
            "New Distribution : [402 300]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "Percentages and Numbers of Paraphrased Rows\n",
            "\t0.2 -> 201\n",
            "\t0.5 -> 501\n",
            "\t0.7 -> 702\n",
            "\t1 -> 969\n",
            "\t2 -> 1938\n",
            "\t5 -> 4845\n",
            "Training Set\n",
            "Dataset Name     : tweets_disaster.csv\n",
            "Total count      : 7613\n",
            "Distribution     : [4342 3271]\n",
            "Current count    : 1894\n",
            "New Distribution : [1080  814]\n",
            "Label Names      : [0, 1]\n",
            "Num of Label     : 2\n",
            "\n",
            "\n",
            "Train Set : 5573\n",
            "Test Set  : 1894\n",
            "Norm Set  : 1001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qOT7Jk4Dp0sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proje Prosedürünün Yürütülmesi - Eğitim ve Test"
      ],
      "metadata": {
        "id": "WGtjyGpI-CA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ds in datasets:\n",
        "  print(\"#######################################################################\\n\"+\n",
        "        \"##\\n\"+\n",
        "        \"##\\t\\tENSEMBLE LEARNING\\n\"+\n",
        "        \"##\\n\"+\n",
        "        \"#######################################################################\\n\\n\")\n",
        "  train_ds_name = f\"train_per{per}_{ds.name}\"\n",
        "  test_ds_name = f\"test_per{per}_{ds.name}\"\n",
        "  norm_ds_name = f\"norm_{ds.name}\"\n",
        "  train_start = ModelTraining(train_ds_name, test_ds_name, norm_ds_name)\n",
        "\n",
        "  for arch in [\"t5\", \"pg\"]:\n",
        "    for p in paraphrase_percent:\n",
        "      phr_ds_name = f\"phr_{arch}_{ds.name}_k{k}_p{p}.csv\"\n",
        "      train_start.start_analysis_for_a_percentage(phr_ds_name, p)\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4vlkOyz-Ncw",
        "outputId": "60405467-e8f2-451b-dcff-5fb13ea63fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#######################################################################\n",
            "##\n",
            "##\t\tENSEMBLE LEARNING\n",
            "##\n",
            "#######################################################################\n",
            "\n",
            "\n",
            "DATASET: test_per0.3_tweets_disaster.csv\n",
            "{'text': ['way', 'cannot', 'eat', 'shit'], 'label': '0'}\n",
            "[('fire', 99), ('like', 90), ('get', 79), ('via', 59), ('bomb', 59), ('year', 57), ('one', 57), ('news', 53), ('new', 51), ('flood', 47), ('kill', 45), ('peopl', 45), ('storm', 45), ('video', 44), ('day', 43), ('go', 41), ('attack', 41), ('time', 41), ('would', 40), ('build', 40), ('love', 39), ('crash', 39), ('look', 38), ('disast', 38), ('come', 38), ('still', 38), ('us', 37), ('malaysia', 37), ('burn', 36), ('evacu', 36)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "################################################################################################\n",
            "\n",
            "[\\_(O.O)_/] train_per0.3_tweets_disaster.csv\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "################################################################################################\n",
            "\n",
            "[\\_(O.O)_/] norm_tweets_disaster.csv\n",
            "\n",
            "\n",
            "DATASET: norm_tweets_disaster.csv\n",
            "{'text': ['draw', 'day', 'demolit', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money', 'lay', 'yo'], 'label': '0'}\n",
            "[('like', 49), ('get', 48), ('fire', 40), ('new', 35), ('news', 33), ('via', 33), ('go', 30), ('kill', 30), ('flood', 30), ('disast', 29), ('bomb', 29), ('emerg', 28), ('peopl', 27), ('one', 25), ('year', 25), ('time', 24), ('love', 24), ('car', 23), ('crash', 23), ('would', 22), ('polic', 22), ('storm', 22), ('attack', 22), ('say', 21), ('home', 21), ('day', 20), ('may', 20), ('devast', 19), ('watch', 19), ('burn', 19)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.722 | Train Accuracy: 56.41%\n",
            "\tTest Loss :0.6943896114826202 | Test Accuracy : 0.5435763890544574\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.588 | Train Accuracy: 68.35%\n",
            "\tTest Loss :0.7300934970378876 | Test Accuracy : 0.5159722223877907\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.529 | Train Accuracy: 74.74%\n",
            "\tTest Loss :0.7641707579294841 | Test Accuracy : 0.5230902778605621\n",
            "\n",
            "# Generated for norm_tweets_disaster.csv\n",
            "train_acc =  [56.41276044771075, 68.34852434694767, 74.7395833954215]\n",
            "test_acc =  [54.35763890544574, 51.597222238779075, 52.309027786056205]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "################################################################################################\n",
            "\n",
            "[\\_(O.O)_/] Önce Norm : phr_pg_tweets_disaster.csv_k5_p1.csv\n",
            "\n",
            "\n",
            "DATASET: norm_tweets_disaster.csv\n",
            "{'text': ['draw', 'day', 'demolit', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money', 'lay', 'yo'], 'label': '0'}\n",
            "[('like', 49), ('get', 48), ('fire', 40), ('new', 35), ('news', 33), ('via', 33), ('go', 30), ('kill', 30), ('flood', 30), ('disast', 29), ('bomb', 29), ('emerg', 28), ('peopl', 27), ('one', 25), ('year', 25), ('time', 24), ('love', 24), ('car', 23), ('crash', 23), ('would', 22), ('polic', 22), ('storm', 22), ('attack', 22), ('say', 21), ('home', 21), ('day', 20), ('may', 20), ('devast', 19), ('watch', 19), ('burn', 19)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.689 | Train Accuracy: 58.18%\n",
            "\tTest Loss :0.6953222249945005 | Test Accuracy : 0.5482638890544573\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.585 | Train Accuracy: 70.25%\n",
            "\tTest Loss :0.7380131532748541 | Test Accuracy : 0.5303819447755813\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.548 | Train Accuracy: 73.85%\n",
            "\tTest Loss :0.7514046102762222 | Test Accuracy : 0.5303819445272286\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.488 | Train Accuracy: 76.00%\n",
            "\tTest Loss :0.777216374874115 | Test Accuracy : 0.532118055721124\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.443 | Train Accuracy: 79.30%\n",
            "\tTest Loss :0.8198304891586303 | Test Accuracy : 0.5310763890544573\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Accuracy: 81.00%\n",
            "\tTest Loss :0.8575520008802414 | Test Accuracy : 0.5112847223877907\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv ÖNCE 1\n",
            "train_acc =  [58.18142369389534, 70.2473958954215, 73.84982639923692, 75.99826389923692, 79.296875, 81.00043404847383]\n",
            "test_acc =  [54.82638890544573, 53.03819447755813, 53.03819445272286, 53.211805572112404, 53.10763890544573, 51.12847223877907]\n",
            "\n",
            "DATASET: phr_pg_tweets_disaster.csv_k5_p1.csv\n",
            "{'text': ['draw', 'day', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money'], 'label': '0'}\n",
            "[('fire', 40), ('get', 37), ('new', 37), ('like', 37), ('go', 34), ('peopl', 29), ('emerg', 25), ('year', 25), ('flood', 24), ('storm', 23), ('kill', 22), ('would', 22), ('attack', 21), ('crash', 21), ('car', 20), ('bomb', 20), ('one', 19), ('want', 19), ('burn', 19), ('man', 19), ('die', 18), ('california', 18), ('love', 17), ('disast', 17), ('us', 16), ('destroy', 16), ('time', 16), ('make', 15), ('say', 15), ('think', 15)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.746 | Train Accuracy: 54.98%\n",
            "\tTest Loss :0.7025564417243004 | Test Accuracy : 0.5619791666666667\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.607 | Train Accuracy: 66.82%\n",
            "\tTest Loss :0.7312064960598945 | Test Accuracy : 0.573090277860562\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.537 | Train Accuracy: 73.58%\n",
            "\tTest Loss :0.7503186623255412 | Test Accuracy : 0.5513888890544574\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.504 | Train Accuracy: 75.71%\n",
            "\tTest Loss :0.7976979325215022 | Test Accuracy : 0.565277777860562\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.495 | Train Accuracy: 75.74%\n",
            "\tTest Loss :0.8413721611102422 | Test Accuracy : 0.5239583333333333\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.454 | Train Accuracy: 80.07%\n",
            "\tTest Loss :0.8601450582345327 | Test Accuracy : 0.5302083333333333\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv ÖNCE 2\n",
            "train_acc =  [54.984319017779434, 66.8234767452363, 73.57750900330082, 75.70564516129032, 75.73924737591898, 80.07392479527381]\n",
            "test_acc =  [56.19791666666667, 57.3090277860562, 55.13888890544574, 56.5277777860562, 52.39583333333333, 53.02083333333333]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "################################################################################################\n",
            "\n",
            "[\\_(O.O)_/] Sonra Norm : phr_pg_tweets_disaster.csv_k5_p1.csv\n",
            "\n",
            "\n",
            "DATASET: phr_pg_tweets_disaster.csv_k5_p1.csv\n",
            "{'text': ['draw', 'day', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money'], 'label': '0'}\n",
            "[('fire', 40), ('get', 37), ('new', 37), ('like', 37), ('go', 34), ('peopl', 29), ('emerg', 25), ('year', 25), ('flood', 24), ('storm', 23), ('kill', 22), ('would', 22), ('attack', 21), ('crash', 21), ('car', 20), ('bomb', 20), ('one', 19), ('want', 19), ('burn', 19), ('man', 19), ('die', 18), ('california', 18), ('love', 17), ('disast', 17), ('us', 16), ('destroy', 16), ('time', 16), ('make', 15), ('say', 15), ('think', 15)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.698 | Train Accuracy: 58.85%\n",
            "\tTest Loss :0.6914149741331737 | Test Accuracy : 0.550868055721124\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.586 | Train Accuracy: 69.90%\n",
            "\tTest Loss :0.7308502500255902 | Test Accuracy : 0.5583333333333333\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.549 | Train Accuracy: 72.73%\n",
            "\tTest Loss :0.7579526906212171 | Test Accuracy : 0.5447916666666667\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.517 | Train Accuracy: 75.19%\n",
            "\tTest Loss :0.8397648786505063 | Test Accuracy : 0.5614583333333333\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.506 | Train Accuracy: 75.53%\n",
            "\tTest Loss :0.839494048555692 | Test Accuracy : 0.5255208333333333\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.458 | Train Accuracy: 79.12%\n",
            "\tTest Loss :0.8536584496498107 | Test Accuracy : 0.5217013890544574\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv SONRA 1\n",
            "train_acc =  [58.84856639369842, 69.90367385648912, 72.72625450165042, 75.19041222910727, 75.52643370243811, 79.1218638420105]\n",
            "test_acc =  [55.0868055721124, 55.833333333333336, 54.479166666666664, 56.14583333333333, 52.552083333333336, 52.17013890544574]\n",
            "\n",
            "DATASET: norm_tweets_disaster.csv\n",
            "{'text': ['draw', 'day', 'demolit', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money', 'lay', 'yo'], 'label': '0'}\n",
            "[('like', 49), ('get', 48), ('fire', 40), ('new', 35), ('news', 33), ('via', 33), ('go', 30), ('kill', 30), ('flood', 30), ('disast', 29), ('bomb', 29), ('emerg', 28), ('peopl', 27), ('one', 25), ('year', 25), ('time', 24), ('love', 24), ('car', 23), ('crash', 23), ('would', 22), ('polic', 22), ('storm', 22), ('attack', 22), ('say', 21), ('home', 21), ('day', 20), ('may', 20), ('devast', 19), ('watch', 19), ('burn', 19)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.740 | Train Accuracy: 57.86%\n",
            "\tTest Loss :0.7354294588168462 | Test Accuracy : 0.5456597223877907\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.608 | Train Accuracy: 67.85%\n",
            "\tTest Loss :0.782196514805158 | Test Accuracy : 0.49583333333333335\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.555 | Train Accuracy: 71.56%\n",
            "\tTest Loss :0.7729558865229289 | Test Accuracy : 0.5201388890544574\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.508 | Train Accuracy: 75.03%\n",
            "\tTest Loss :0.7967226058244705 | Test Accuracy : 0.5222222223877907\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.481 | Train Accuracy: 77.29%\n",
            "\tTest Loss :0.8318252111474673 | Test Accuracy : 0.532465277860562\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.450 | Train Accuracy: 78.50%\n",
            "\tTest Loss :0.8622091680765152 | Test Accuracy : 0.5038194447755814\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv SONRA 2\n",
            "train_acc =  [57.855902798473835, 67.84939244389534, 71.56032994389534, 75.0325521454215, 77.28949654847383, 78.50477434694767]\n",
            "test_acc =  [54.56597223877907, 49.583333333333336, 52.01388890544574, 52.22222223877907, 53.2465277860562, 50.381944477558136]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "################################################################################################\n",
            "\n",
            "[\\_(O.O)_/] Arada : phr_pg_tweets_disaster.csv_k5_p1.csv\n",
            "\n",
            "\n",
            "DATASET: arada_ilk.csv\n",
            "{'text': ['possibl', 'see', 'u', 'night', 'wee', 'barra', 'get', 'absolut', 'wreck'], 'label': '0'}\n",
            "[('new', 23), ('like', 17), ('fire', 17), ('get', 14), ('go', 13), ('one', 13), ('emerg', 13), ('attack', 12), ('love', 12), ('bomb', 12), ('time', 12), ('california', 12), ('want', 12), ('car', 12), ('make', 11), ('us', 11), ('flood', 11), ('peopl', 11), ('year', 11), ('say', 10), ('hous', 10), ('destroy', 10), ('take', 10), ('northern', 10), ('would', 10), ('crash', 10), ('train', 10), ('live', 10), ('wreck', 9), ('use', 9)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.727 | Train Accuracy: 54.30%\n",
            "\tTest Loss :0.7182750642299652 | Test Accuracy : 0.5640625\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.595 | Train Accuracy: 65.62%\n",
            "\tTest Loss :0.717508923014005 | Test Accuracy : 0.5295138890544574\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.558 | Train Accuracy: 73.24%\n",
            "\tTest Loss :0.8338773307700952 | Test Accuracy : 0.565625\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.509 | Train Accuracy: 74.61%\n",
            "\tTest Loss :0.77404878338178 | Test Accuracy : 0.50625\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv ARADA 1\n",
            "train_acc =  [54.296875, 65.625, 73.2421875, 74.609375]\n",
            "test_acc =  [56.40625, 52.95138890544574, 56.56250000000001, 50.625]\n",
            "\n",
            "DATASET: norm_tweets_disaster.csv\n",
            "{'text': ['draw', 'day', 'demolit', 'daili', 'footbal', 'select', 'servic', 'consist', 'make', 'money', 'lay', 'yo'], 'label': '0'}\n",
            "[('like', 49), ('get', 48), ('fire', 40), ('new', 35), ('news', 33), ('via', 33), ('go', 30), ('kill', 30), ('flood', 30), ('disast', 29), ('bomb', 29), ('emerg', 28), ('peopl', 27), ('one', 25), ('year', 25), ('time', 24), ('love', 24), ('car', 23), ('crash', 23), ('would', 22), ('polic', 22), ('storm', 22), ('attack', 22), ('say', 21), ('home', 21), ('day', 20), ('may', 20), ('devast', 19), ('watch', 19), ('burn', 19)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.734 | Train Accuracy: 56.14%\n",
            "\tTest Loss :0.719716839492321 | Test Accuracy : 0.5517361111938953\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.582 | Train Accuracy: 70.59%\n",
            "\tTest Loss :0.7569190214077631 | Test Accuracy : 0.5152777781089147\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.538 | Train Accuracy: 73.57%\n",
            "\tTest Loss :0.7687923833727837 | Test Accuracy : 0.5189236111938953\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.505 | Train Accuracy: 75.31%\n",
            "\tTest Loss :0.790241676568985 | Test Accuracy : 0.5284722223877907\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv ARADA 2\n",
            "train_acc =  [56.14149309694767, 70.59461809694767, 73.5677083954215, 75.31467014923692]\n",
            "test_acc =  [55.173611119389534, 51.52777781089147, 51.89236111938953, 52.84722223877907]\n",
            "\n",
            "DATASET: arada_son.csv\n",
            "{'text': ['lightn', 'strike', 'chang', 'rock', 'atom', 'level'], 'label': '0'}\n",
            "[('get', 23), ('fire', 23), ('go', 21), ('like', 20), ('peopl', 18), ('storm', 15), ('kill', 14), ('new', 14), ('year', 14), ('flood', 13), ('would', 12), ('emerg', 12), ('crash', 11), ('burn', 11), ('man', 11), ('know', 10), ('lot', 10), ('die', 10), ('think', 9), ('play', 9), ('attack', 9), ('come', 9), ('disast', 9), ('may', 9), ('back', 8), ('bomb', 8), ('work', 8), ('car', 8), ('stop', 8), ('book', 8)]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.786 | Train Accuracy: 54.53%\n",
            "\tTest Loss :0.7678578997651736 | Test Accuracy : 0.4921875\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.639 | Train Accuracy: 65.55%\n",
            "\tTest Loss :0.7125129908323288 | Test Accuracy : 0.559375\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.577 | Train Accuracy: 68.98%\n",
            "\tTest Loss :0.714088981350263 | Test Accuracy : 0.5375\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.495 | Train Accuracy: 79.10%\n",
            "\tTest Loss :0.7650989254315694 | Test Accuracy : 0.5782986111938954\n",
            "\n",
            "# Generated for phr_pg_tweets_disaster.csv_k5_p1.csv ARADA 3\n",
            "train_acc =  [54.53125014901161, 65.5468750745058, 68.98437514901161, 79.1015625]\n",
            "test_acc =  [49.21875, 55.93749999999999, 53.75, 57.829861119389534]\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Kolektif Öğrenme - Yapay Örnek Artırımı.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NDONXap9Z0LI",
        "y_WQcj78IaYt",
        "nE995QzZqhMX",
        "lkG4ilk8BEnq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}